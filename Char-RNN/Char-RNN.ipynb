{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN\n",
    "This example is a character-level Recurrent Neural Network. We intended to feeding the network some sequence of character and it should predict the next character in the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import helper\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from helper import txt_reader, nn_layer_util\n",
    "import os,sys\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Try to load some data from <b>input.txt</b>. Using helper functions in <b>txt_reader</b>, the file was converted into a list of words.\n",
    "After that, we calculate the total number of unique words in the dataset. To convenient the Neural Network, we then convert every words in the dataset into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 109892, Num vocabulary : 77\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/oneday/input.txt\"\n",
    "\n",
    "# convert txt to list of characters, type=str\n",
    "data_char = helper.txt_reader.txt2char(data_path)\n",
    "# calculating the numbers of unique characters in the list\n",
    "chars, vocab_size = helper.txt_reader.unique_element(data_char)\n",
    "print \"Total chars: {}, Num vocabulary : {}\".format(len(data_char), vocab_size)\n",
    "\n",
    "# building a map between chars & indices\n",
    "chars_indices = dict((w,i) for i, w in enumerate(chars))\n",
    "indices_chars = dict((i,w) for i, w in enumerate(chars))\n",
    "# converting the dataset to indices\n",
    "data_idx = [chars_indices[w] for w in data_char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print data\n",
    "Try to print some data to check the convertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id.\r\n",
      "\r\n",
      "\t'Ah - you mean we have to change the world?' the boy replied.\r\n",
      "\r\n",
      "\t'No, not all of the world,\n",
      "[59, 54, 12, 3, 2, 3, 2, 1, 7, 26, 58, 4, 11, 4, 75, 65, 71, 4, 63, 55, 51, 64, 4, 73, 55, 4, 58, 51, 72, 55, 4, 70, 65, 4, 53, 58, 51, 64, 57, 55, 4, 70, 58, 55, 4, 73, 65, 68, 62, 54, 25, 7, 4, 70, 58, 55, 4, 52, 65, 75, 4, 68, 55, 66, 62, 59, 55, 54, 12, 3, 2, 3, 2, 1, 7, 39, 65, 10, 4, 64, 65, 70, 4, 51, 62, 62, 4, 65, 56, 4, 70, 58, 55, 4, 73, 65, 68, 62, 54, 10]\n"
     ]
    }
   ],
   "source": [
    "print data_char[200:300]\n",
    "print data_idx[200:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 64 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables\n",
    "\n",
    "Everytime we execute the TensorFlow computational graph, we can feed different values to the Placeholder variables. These Placeholder variable are multi-dimensional array called <b>tensor</b> and the data-type is set to <b>float32</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output: [seq_length, vocab_size]\n",
    "x = tf.placeholder(tf.float32, shape=[seq_length, vocab_size], name='x')\n",
    "x2 = tf.placeholder(tf.float32, shape=[2, vocab_size], name='x_test')\n",
    "# Label y associated with X \n",
    "# Output: [seq_length, vocab_size]\n",
    "# Each example's class is represent in vector e.g. For class 5 = [0,0,0,0,0,1,0,0,0,0]\n",
    "y_true = tf.placeholder(tf.float32, shape=[seq_length, vocab_size], name='y_true')\n",
    "y2 = tf.placeholder(tf.float32, shape=[2, vocab_size], name='y_test')\n",
    "\n",
    "# Class-number y associated with X \n",
    "# Output: [seq_length]\n",
    "# Each example's class is represent in number e.g. [5]\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some tensorflow variables\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "hprev = tf.Variable(tf.zeros(shape=(hidden_size, seq_length), name='hprev', dtype=tf.float32), trainable=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function - RNN layer\n",
    "Create a new rnn layer, using the function: W*x + W*(x-1) + b\n",
    "<br><b>hprev</b> is the output of the function from previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_rnn_layer(inputs,\n",
    "                  hprev,\n",
    "                  vocab_size,\n",
    "                  hidden_size,\n",
    "                  batch_size = 32):\n",
    "    with tf.variable_scope('rnn_layer') as scope:\n",
    "        # Creating a simple rnn cell\n",
    "        # Create new weights, for filters with the given shape.\n",
    "        weights_x = helper.nn_layer_util.new_weights(shape=(vocab_size, hidden_size))\n",
    "        weights_h = helper.nn_layer_util.new_weights(shape=(hidden_size, hidden_size))\n",
    "        weights_y = helper.nn_layer_util.new_weights(shape=(hidden_size, vocab_size))\n",
    "\n",
    "        # Create new biases, one for each filter.\n",
    "        biases_h = helper.nn_layer_util.new_biases(length=hidden_size)\n",
    "        biases_y = helper.nn_layer_util.new_biases(length=vocab_size)\n",
    "        \n",
    "        hyp, outputs = {}, {}\n",
    "        hyp[-1] = np.copy(hprev)\n",
    "        layer = tf.matmul(inputs, weights_x) + tf.transpose(tf.matmul(weights_h, hprev))\n",
    "        hyp = tf.tanh(tf.nn.bias_add(layer, biases_h))\n",
    "        outputs = tf.nn.bias_add(tf.matmul(hyp, weights_y), biases_y)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function - Get batch sequence\n",
    "Since we are not feeding the whole data file into the RNN at once. Therefore, we are going feed batches with size of <b>seq_length</b> to the RNN each time. Similar to CNN, instead of feeding an image and predict its class, we feed a character from the sequence at once to predict the next character in the sequence. That's why, the <b>y_batch</b> will be starting from <font color=\"red\">p+1 ~ p+seq_length+1</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 0 # Global seq. counter p\n",
    "def get_batch(hidden_size, data, seq_length, d2i):\n",
    "    global p\n",
    "    # Number of images in the training-set.\n",
    "    if p+seq_length+1 >= len(data) or global_step == 0:\n",
    "        hprev = np.zeros((hidden_size, 1)) # Reset RNN memory\n",
    "        p = 0 # Go to the start of data\n",
    "    x_tmp = [d2i[i] for i in data[p:p+seq_length]]\n",
    "    y_tmp = [d2i[i] for i in data[p+1:p+seq_length+1]]\n",
    "    x_batch = np.zeros((seq_length, vocab_size))\n",
    "    y_batch = np.zeros((seq_length, vocab_size))\n",
    "    x_batch[np.arange(seq_length), x_tmp] = 1\n",
    "    y_batch[np.arange(seq_length), y_tmp] = 1\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture (RNN part)\n",
    "This part is the implementation of the whole RNN.\n",
    "\n",
    "CNN-Architecture: <br>\n",
    "RNN - Output(Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN layer\n",
    "layer_rnn = new_rnn_layer(inputs=x, hprev= hprev, vocab_size=vocab_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Class vector e.g. For class 5 = [0,0,0,0,0,1,0,0,0,0] \n",
    "y_pred = tf.nn.softmax(layer_rnn)\n",
    "# Use argmax to convert y from class vector to class labels e.g. 5\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture (Optimisation Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-function to be optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cost:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Logistic Cost Function\n",
    "# - [y * log(h(x)) + (1-y) * log(1-h(x))]\n",
    "# A cost is output for each image\n",
    "# Output: [num of image,]\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_rnn,labels=y_true)\n",
    "# Computes the mean of all cost, resulting a single value\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "# Collecting accuracy for TensorBoard\n",
    "tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method\n",
    "AdamOptimizer which is an advanced form of Gradient Descent we can use for minimise the cost.\n",
    "<br>\n",
    "**Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures\n",
    "We need a few more performance measures to display the progress to the user.\n",
    "This is a vector of booleans whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector of booleans whether the predicted class equals the true class of each image.\n",
    "# Output: [num of image,]\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "# Cast booleans to floats, False = 0, True = 1\n",
    "# Calculate the average of these number\n",
    "# Output: float32\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Collecting accuracy for TensorBoard\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver\n",
    "Save variables of the neural network to reloaded quickly without having to train the network again.\n",
    "<br>Doc: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb\n",
    "\n",
    "**Note that nothing is actually saved at this point, which will be done further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (Execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow Session to execute the TensorFlow graph\n",
    "session = tf.Session()\n",
    "# Collecting accuracy for TensorBoard\n",
    "# Summaries such as scalars can be seen by running the command below\n",
    "# tensorboard --logdir=\"./log\"\n",
    "# http://192.168.0.150:6006\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./log', session.graph)\n",
    "# Initialise weights and bias\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing-function - Perform optimization iterations\n",
    "This function is called to execute the training process of RNN. A number of optimization iterations so as to gradually improve the variables of the network layers. Each iteration, new batch of data is selected from the training-set and TensorFlow executes the optimizer using them. \n",
    "\n",
    "Input:<br>\n",
    "num_iterations - Number of optimization iterations<br>\n",
    "Output:<br>\n",
    "None<br>\n",
    "*weights is updated with-in the model and message of <b>Training Accuracy</b> is printed every 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "def optimize(num_iterations):\n",
    "    global loss\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch - batch of image\n",
    "        # y_true_batch - labels of x_batch\n",
    "        x_batch, y_true_batch = get_batch(hidden_size=hidden_size,\n",
    "                                          data=data_char, \n",
    "                                          seq_length=seq_length, \n",
    "                                          d2i=chars_indices)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "        \n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        # !!! Add [merged, opt] for TensorBoard !!! Very Important, Order has no effect\n",
    "        layer_pred ,layer, y_p, yt, yc, ty_output, i_global, summary, _ = session.run([y_pred ,layer_rnn ,correct_prediction, y_true_cls, y_pred_cls, layer_rnn, global_step, merged, optimizer], feed_dict=feed_dict_train)\n",
    "        # useless stuff just for testing\n",
    "        prob = np.exp(ty_output) / np.sum(np.exp(ty_output))\n",
    "        pp = prob[np.arange(seq_length),yt]\n",
    "        loss += np.sum(-np.log(pp)) * 0.001\n",
    "        # Add summary to TensorBoard\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "        # Print status to screen every 100 iterations (and last).\n",
    "        if (i_global % 100 == 0) or (i == num_iterations - 1):\n",
    "            # Calculate the accuracy on the training-batch.\n",
    "            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Print status.\n",
    "            msg = \"Global Step: {0:>6}, Training Batch Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i_global, batch_acc))\n",
    "        # Save a checkpoint to disk every 1000 iterations (and last).\n",
    "        if (i_global % 1000 == 0) or (i == num_iterations - 1):\n",
    "            # Save all variables of the TensorFlow graph to a\n",
    "            # checkpoint. Append the global_step counter\n",
    "            # to the filename so we save the last several checkpoints.\n",
    "            saver.save(session,\n",
    "                       save_path=save_path,\n",
    "                       global_step=global_step)\n",
    "\n",
    "            print(\"Saved checkpoint.\")\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore or initialize variables\n",
    "Training this neural network may take a long time, especially if you do not have a GPU. We therefore save checkpoints during training so we can continue training at another time (e.g. during the night), and also for performing analysis later without having to train the neural network every time we want to use it.\n",
    "\n",
    "If you want to restart the training of the neural network, you have to delete the checkpoints first.\n",
    "\n",
    "This is the directory used for the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "Failed to restore checkpoint. Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'checkpoints/'\n",
    "# Create directory if not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'cifar10_cnn')\n",
    "\n",
    "# Try to restore the latest checkpoint. \n",
    "# If checkpoint doesn't exist or TensorFlow graph has been modified, exception will raise.\n",
    "# When exception appears, initialise will be made\n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "    # Use TensorFlow to find the latest checkpoint - if any.\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "\n",
    "    # Try and load the data in the checkpoint.\n",
    "    saver.restore(session, save_path=last_chk_path)\n",
    "\n",
    "    # If we get to this point, the checkpoint was successfully loaded.\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    # If the above failed for some reason, simply\n",
    "    # initialise all the variables for the TensorFlow graph.\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step:    100, Training Batch Accuracy:  26.6%\n",
      "Global Step:    200, Training Batch Accuracy:  65.6%\n",
      "Global Step:    300, Training Batch Accuracy:  53.1%\n",
      "Global Step:    400, Training Batch Accuracy:  57.8%\n",
      "Global Step:    500, Training Batch Accuracy:  73.4%\n",
      "Global Step:    600, Training Batch Accuracy: 100.0%\n",
      "Global Step:    700, Training Batch Accuracy: 100.0%\n",
      "Global Step:    800, Training Batch Accuracy: 100.0%\n",
      "Global Step:    900, Training Batch Accuracy: 100.0%\n",
      "Global Step:   1000, Training Batch Accuracy: 100.0%\n",
      "Saved checkpoint.\n",
      "Time usage: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
