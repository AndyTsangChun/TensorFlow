{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from PIL import Image as pilImage\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from datetime import timedelta\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables\n",
    "\n",
    "Everytime we execute the TensorFlow computational graph, we can feed different values to the Placeholder variables. These Placeholder variable are multi-dimensional array called <b>tensor</b> and the data-type is set to <b>float32</b>. \n",
    "\n",
    "<font color=\"red\">**None means it can hold an arbitrary number of images.</font>\n",
    "<br>The images input shape: [None, img_size, img_size, num_channels]\n",
    "<br>- Each image being <b>img_size</b> pixels high and <b>img_size</b> pixels wide and with <b>num_channels</b> colour channels.\n",
    "<br>The labels shape: [None, num_classes]\n",
    "<br>The class shape: [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow expect 4-dim input X, so we have to reshape x\n",
    "# Output: [arbitrary, img_height, img_width, num_channels]\n",
    "# Note that all input is square, thus, img_height == img_width == img_size\n",
    "# num_images can be inferred automatically by using -1 \n",
    "x = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS], name='x')\n",
    "\n",
    "# ConfidenceMap y associated with X \n",
    "# Output: [arbitrary, ????]\n",
    "y_true_cm = tf.placeholder(tf.float32, shape=[None, ???], name='y_true_cm')\n",
    "\n",
    "# Affinity field y associated with X \n",
    "# Output: [arbitrary, ????]\n",
    "y_true_af = tf.placeholder(tf.float32, shape=[None, ???], name='y_true_af')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_1, w1_1 = cnn.new_conv_layer(input=x, num_input_channels=3, filter_size=3, num_filters=64, padding='SAME')\n",
    "conv1_2, w1_2 = cnn.new_conv_layer(input=conv1_1, num_input_channels=3, filter_size=3, num_filters=64, padding='SAME', use_pooling=True)\n",
    "conv2_1, w2_1 = cnn.new_conv_layer(input=conv1_2, num_input_channels=64, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv2_2, w2_2 = cnn.new_conv_layer(input=conv2_1, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME', use_pooling=True)\n",
    "conv3_1, w3_1 = cnn.new_conv_layer(input=conv2_2, num_input_channels=128, filter_size=3, num_filters=256, padding='SAME')\n",
    "conv3_2, w3_2 = cnn.new_conv_layer(input=conv3_1, num_input_channels=256, filter_size=3, num_filters=256, padding='SAME')\n",
    "conv3_3, w3_3 = cnn.new_conv_layer(input=conv3_2, num_input_channels=256, filter_size=3, num_filters=256, padding='SAME')\n",
    "conv3_4, w3_4 = cnn.new_conv_layer(input=conv3_3, num_input_channels=256, filter_size=3, num_filters=256, padding='SAME', use_pooling=True)\n",
    "conv4_1, w4_1 = cnn.new_conv_layer(input=conv3_4, num_input_channels=256, filter_size=3, num_filters=512, padding='SAME')\n",
    "conv4_2, w4_2 = cnn.new_conv_layer(input=conv4_1, num_input_channels=512, filter_size=3, num_filters=512, padding='SAME')\n",
    "conv4_3, w4_3 = cnn.new_conv_layer(input=conv4_2, num_input_channels=512, filter_size=3, num_filters=256, padding='SAME')\n",
    "conv4_4, w4_4 = cnn.new_conv_layer(input=conv4_3, num_input_channels=256, filter_size=3, num_filters=128, padding='SAME')\n",
    "# stage 1 branch 1\n",
    "conv1_1_1, w1_1_1 = cnn.new_conv_layer(input=conv4_4, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_2_1, w1_2_1 = cnn.new_conv_layer(input=conv1_1_1, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_3_1, w1_3_1 = cnn.new_conv_layer(input=conv1_2_1, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_4_1, w1_4_1 = cnn.new_conv_layer(input=conv1_3_1, num_input_channels=128, filter_size=1, num_filters=512, padding='SAME')\n",
    "conv1_5_1, w1_5_1 = cnn.new_conv_layer(input=conv1_4_1, num_input_channels=512, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 1 branch 2\n",
    "conv1_1_2, w1_1_2 = cnn.new_conv_layer(input=conv4_4, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_2_2, w1_2_2 = cnn.new_conv_layer(input=conv1_1_2, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_3_2, w1_3_2 = cnn.new_conv_layer(input=conv1_2_2, num_input_channels=128, filter_size=3, num_filters=128, padding='SAME')\n",
    "conv1_4_2, w1_4_2 = cnn.new_conv_layer(input=conv1_3_2, num_input_channels=128, filter_size=1, num_filters=512, padding='SAME')\n",
    "conv1_5_2, w1_5_2 = cnn.new_conv_layer(input=conv1_4_2, num_input_channels=512, filter_size=1, num_filters=19, padding='SAME', use_relu=False)\n",
    "# concat 1\n",
    "concat1 = tf.concat([conv1_5_1, conv1_5_2], 1)\n",
    "# stage 2 branch 1\n",
    "conv2_1_1, w2_1_1 = cnn.new_conv_layer(input=concat1, num_input_channels=38, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_2_1, w2_2_1 = cnn.new_conv_layer(input=conv2_1_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_3_1, w2_3_1 = cnn.new_conv_layer(input=conv2_2_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_4_1, w2_4_1 = cnn.new_conv_layer(input=conv2_3_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_5_1, w2_5_1 = cnn.new_conv_layer(input=conv2_4_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_6_1, w2_6_1 = cnn.new_conv_layer(input=conv2_5_1, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv2_7_1, w2_7_1 = cnn.new_conv_layer(input=conv2_6_1, num_input_channels=128, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 2 branch 2\n",
    "conv2_1_2, w2_1_2 = cnn.new_conv_layer(input=concat1, num_input_channels=19, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_2_2, w2_2_2 = cnn.new_conv_layer(input=conv2_1_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_3_2, w2_3_2 = cnn.new_conv_layer(input=conv2_2_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_4_2, w2_4_2 = cnn.new_conv_layer(input=conv2_3_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_5_2, w2_5_2 = cnn.new_conv_layer(input=conv2_4_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv2_6_2, w2_6_2 = cnn.new_conv_layer(input=conv2_5_2, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv2_7_2, w2_7_2 = cnn.new_conv_layer(input=conv2_6_2, num_input_channels=128, filter_size=1, num_filters=19, padding='SAME', use_relu=False)\n",
    "# concat 2\n",
    "concat2 = tf.concat([conv2_7_1, conv2_7_2], 1)\n",
    "# stage 3 branch 1\n",
    "conv3_1_1, w3_1_1 = cnn.new_conv_layer(input=concat2, num_input_channels=38, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_2_1, w3_2_1 = cnn.new_conv_layer(input=conv3_1_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_3_1, w3_3_1 = cnn.new_conv_layer(input=conv3_2_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_4_1, w3_4_1 = cnn.new_conv_layer(input=conv3_3_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_5_1, w3_5_1 = cnn.new_conv_layer(input=conv3_4_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_6_1, w3_6_1 = cnn.new_conv_layer(input=conv3_5_1, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv3_7_1, w3_7_1 = cnn.new_conv_layer(input=conv3_6_1, num_input_channels=128, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 3 branch 2\n",
    "conv3_1_2, w3_1_2 = cnn.new_conv_layer(input=concat2, num_input_channels=19, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_2_2, w3_2_2 = cnn.new_conv_layer(input=conv3_1_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_3_2, w3_3_2 = cnn.new_conv_layer(input=conv3_2_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_4_2, w3_4_2 = cnn.new_conv_layer(input=conv3_3_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_5_2, w3_5_2 = cnn.new_conv_layer(input=conv3_4_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv3_6_2, w3_6_2 = cnn.new_conv_layer(input=conv3_5_2, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv3_7_2, w3_7_2 = cnn.new_conv_layer(input=conv3_6_2, num_input_channels=128, filter_size=1, num_filters=19, padding='SAME', use_relu=False)\n",
    "# concat 3\n",
    "concat3 = tf.concat([conv3_7_1, conv3_7_2], 1)\n",
    "# stage 4 branch 1\n",
    "conv4_1_1, w4_1_1 = cnn.new_conv_layer(input=concat3, num_input_channels=38, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_2_1, w4_2_1 = cnn.new_conv_layer(input=conv4_1_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_3_1, w4_3_1 = cnn.new_conv_layer(input=conv4_2_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_4_1, w4_4_1 = cnn.new_conv_layer(input=conv4_3_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_5_1, w4_5_1 = cnn.new_conv_layer(input=conv4_4_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_6_1, w4_6_1 = cnn.new_conv_layer(input=conv4_5_1, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv4_7_1, w4_7_1 = cnn.new_conv_layer(input=conv4_6_1, num_input_channels=128, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 4 branch 2\n",
    "conv4_1_2, w4_1_2 = cnn.new_conv_layer(input=concat3, num_input_channels=19, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_2_2, w4_2_2 = cnn.new_conv_layer(input=conv4_1_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_3_2, w4_3_2 = cnn.new_conv_layer(input=conv4_2_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_4_2, w4_4_2 = cnn.new_conv_layer(input=conv4_3_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_5_2, w4_5_2 = cnn.new_conv_layer(input=conv4_4_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv4_6_2, w4_6_2 = cnn.new_conv_layer(input=conv4_5_2, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv4_7_2, w4_7_2 = cnn.new_conv_layer(input=conv4_6_2, num_input_channels=128, filter_size=1, num_filters=19, padding='SAME', use_relu=False)\n",
    "# concat 4\n",
    "concat4 = tf.concat([conv4_7_1, conv4_7_2], 1)\n",
    "# stage 5 branch 1\n",
    "conv5_1_1, w5_1_1 = cnn.new_conv_layer(input=concat4, num_input_channels=38, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_2_1, w5_2_1 = cnn.new_conv_layer(input=conv5_1_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_3_1, w5_3_1 = cnn.new_conv_layer(input=conv5_2_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_4_1, w5_4_1 = cnn.new_conv_layer(input=conv5_3_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_5_1, w5_5_1 = cnn.new_conv_layer(input=conv5_4_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_6_1, w5_6_1 = cnn.new_conv_layer(input=conv5_5_1, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv5_7_1, w5_7_1 = cnn.new_conv_layer(input=conv5_6_1, num_input_channels=128, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 5 branch 2\n",
    "conv5_1_2, w5_1_2 = cnn.new_conv_layer(input=concat4, num_input_channels=19, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_2_2, w5_2_2 = cnn.new_conv_layer(input=conv5_1_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_3_2, w5_3_2 = cnn.new_conv_layer(input=conv5_2_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_4_2, w5_4_2 = cnn.new_conv_layer(input=conv5_3_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_5_2, w5_5_2 = cnn.new_conv_layer(input=conv5_4_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv5_6_2, w5_6_2 = cnn.new_conv_layer(input=conv5_5_2, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv5_7_2, w5_7_2 = cnn.new_conv_layer(input=conv5_6_2, num_input_channels=128, filter_size=1, num_filters=19, padding='SAME', use_relu=False)\n",
    "# concat 5\n",
    "concat5 = tf.concat([conv5_7_1, conv5_7_2], 1)\n",
    "# stage 6 branch 1\n",
    "conv6_1_1, w6_1_1 = cnn.new_conv_layer(input=concat5, num_input_channels=38, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_2_1, w6_2_1 = cnn.new_conv_layer(input=conv6_1_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_3_1, w6_3_1 = cnn.new_conv_layer(input=conv6_2_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_4_1, w6_4_1 = cnn.new_conv_layer(input=conv6_3_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_5_1, w6_5_1 = cnn.new_conv_layer(input=conv6_4_1, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_6_1, w6_6_1 = cnn.new_conv_layer(input=conv6_5_1, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv6_7_1, w6_7_1 = cnn.new_conv_layer(input=conv6_6_1, num_input_channels=128, filter_size=1, num_filters=38, padding='SAME', use_relu=False)\n",
    "# stage 6 branch 2\n",
    "conv6_1_2, w6_1_2 = cnn.new_conv_layer(input=concat5, num_input_channels=19, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_2_2, w6_2_2 = cnn.new_conv_layer(input=conv6_1_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_3_2, w6_3_2 = cnn.new_conv_layer(input=conv6_2_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_4_2, w6_4_2 = cnn.new_conv_layer(input=conv6_3_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_5_2, w6_5_2 = cnn.new_conv_layer(input=conv6_4_2, num_input_channels=128, filter_size=7, num_filters=128, padding='SAME')\n",
    "conv6_6_2, w6_6_2 = cnn.new_conv_layer(input=conv6_5_2, num_input_channels=128, filter_size=1, num_filters=128, padding='SAME')\n",
    "conv6_7_2, w6_7_2 = cnn.new_conv_layer(input=conv6_6_2, num_input_channels=128, filter_size=1, num_filters=19, padding='SAME', use_relu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture (Optimisation Part)\n",
    "### L2 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b1_loss = tf.nn.l2_loss(w6_7_1 - y_true_cm)\n",
    "b1_loss = tf.reduce_mean(tf.square(w6_7_1 - y_true_cm))\n",
    "#b2_loss = tf.nn.l2_loss(w6_7_2 - y_true_af)\n",
    "tf.reduce_mean(tf.square(w6_7_2 - y_true_af))\n",
    "tf.summary.scalar(\"b1 loss\", b1_loss)\n",
    "tf.summary.scalar(\"b2 loss\", b2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method\n",
    "AdamOptimizer which is an advanced form of Gradient Descent we can use for minimise the cost.\n",
    "<br>\n",
    "**Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(b1_loss, b2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver\n",
    "Save variables of the neural network to reloaded quickly without having to train the network again.\n",
    "<br>Ref: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb\n",
    "\n",
    "**Note that nothing is actually saved at this point, which will be done further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (Execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TensorFlow Session to execute the TensorFlow graph\n",
    "session = tf.Session()\n",
    "# Collecting accuracy for TensorBoard\n",
    "# Summaries such as scalars can be seen by running the command below\n",
    "# tensorboard --logdir=\"./log\"\n",
    "# http://192.168.0.150:6006\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./log', session.graph)\n",
    "# Initialise weights and bias\n",
    "#session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing-function - Perform optimization iterations\n",
    "This function is called to execute the training process of CNN. A number of optimization iterations so as to gradually improve the variables of the network layers. Each iteration, new batch of data is selected from the training-set and TensorFlow executes the optimizer using them. \n",
    "\n",
    "Input:<br>\n",
    "num_iterations - Number of optimization iterations<br>\n",
    "Output:<br>\n",
    "None<br>\n",
    "*weights is updated with-in the model and message of <b>Training Accuracy</b> is printed every 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch - batch of image\n",
    "        # y_true_batch - labels of x_batch\n",
    "        x_batch, y_true_batch = random_batch()\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        # !!! Add [merged, opt] for TensorBoard !!! Very Important, Order has no effect\n",
    "        i_global, summary, _ = session.run([global_step, merged, optimizer], feed_dict=feed_dict_train)\n",
    "        # Add summary to TensorBoard\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "        # Print status to screen every 100 iterations (and last).\n",
    "        if (i_global % 100 == 0) or (i == num_iterations - 1):\n",
    "            # Calculate the accuracy on the training-batch.\n",
    "            batch_acc = session.run(accuracy,\n",
    "                                    feed_dict=feed_dict_train)\n",
    "\n",
    "            # Print status.\n",
    "            msg = \"Global Step: {0:>6}, Training Batch Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i_global, batch_acc))\n",
    "        # Save a checkpoint to disk every 1000 iterations (and last).\n",
    "        if (i_global % 1000 == 0) or (i == num_iterations - 1):\n",
    "            # Save all variables of the TensorFlow graph to a\n",
    "            # checkpoint. Append the global_step counter\n",
    "            # to the filename so we save the last several checkpoints.\n",
    "            saver.save(session,\n",
    "                       save_path=save_path,\n",
    "                       global_step=global_step)\n",
    "\n",
    "            print(\"Saved checkpoint.\")\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore or initialize variables\n",
    "Training this neural network may take a long time, especially if you do not have a GPU. We therefore save checkpoints during training so we can continue training at another time (e.g. during the night), and also for performing analysis later without having to train the neural network every time we want to use it.\n",
    "\n",
    "If you want to restart the training of the neural network, you have to delete the checkpoints first.\n",
    "\n",
    "This is the directory used for the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = 'checkpoints/'\n",
    "# Create directory if not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'att_cnn')\n",
    "\n",
    "# Try to restore the latest checkpoint. \n",
    "# If checkpoint doesn't exist or TensorFlow graph has been modified, exception will raise.\n",
    "# When exception appears, initialise will be made\n",
    "try:\n",
    "    print(\"Trying to restore last checkpoint ...\")\n",
    "\n",
    "    # Use TensorFlow to find the latest checkpoint - if any.\n",
    "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "\n",
    "    # Try and load the data in the checkpoint.\n",
    "    saver.restore(session, save_path=last_chk_path)\n",
    "\n",
    "    # If we get to this point, the checkpoint was successfully loaded.\n",
    "    print(\"Restored checkpoint from:\", last_chk_path)\n",
    "except:\n",
    "    # If the above failed for some reason, simply\n",
    "    # initialise all the variables for the TensorFlow graph.\n",
    "    print(\"Failed to restore checkpoint. Initializing variables instead.\")\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimize(num_iterations=900)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
